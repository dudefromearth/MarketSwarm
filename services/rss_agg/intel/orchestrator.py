#!/usr/bin/env python3
import asyncio
import json
import os
import time

import redis.asyncio as redis

# Primary ingestor
from .ingestor import ingest_feeds

# Publisher (takes only publish_dir)
from .publisher import generate_all_feeds

# Enrichment + raw fetch + canonical
from .article_ingestor import enrich_articles
from .article_fetcher import fetch_and_store_article
from .canonical_fetcher import canonical_fetcher_run_once
from .article_enricher import enrich_articles_lifo


# ------------------------------------------------------------
# Pipeline mode switch
# ------------------------------------------------------------
PIPELINE_MODE = os.getenv("PIPELINE_MODE", "full").lower()

# NEW: propagate FORCE_INGEST into orchestrator logging
FORCE_INGEST = os.getenv("FORCE_INGEST", "false").lower() == "true"


# ------------------------------------------------------------
# Load feeds.json
# ------------------------------------------------------------
async def load_feeds_config():
    feeds_path = os.getenv("FEEDS_CONFIG")

    if feeds_path and os.path.exists(feeds_path):
        print(f"üìò Loaded feeds.json from env: {feeds_path}")
        with open(feeds_path, "r") as f:
            return json.load(f)

    local_path = os.path.join(os.getcwd(), "schema", "feeds.json")
    if os.path.exists(local_path):
        print(f"üìò Loaded feeds.json from local: {local_path}")
        with open(local_path, "r") as f:
            return json.load(f)

    docker_path = "/app/schema/feeds.json"
    if os.path.exists(docker_path):
        print(f"üìò Loaded feeds.json from docker: {docker_path}")
        with open(docker_path, "r") as f:
            return json.load(f)

    raise FileNotFoundError("‚ùå feeds.json not found.")


# ------------------------------------------------------------
# Publisher Scheduler
# ------------------------------------------------------------
async def schedule_feed_generation(publish_dir: str, interval_sec: int):
    while True:
        try:
            print(f"üß© Generating all RSS feeds into: {publish_dir}")
            await asyncio.to_thread(generate_all_feeds, publish_dir)
            print("‚úÖ Feed generation complete.")
        except Exception as e:
            print(f"[Publisher Error] {e}")

        await asyncio.sleep(interval_sec)


# ------------------------------------------------------------
# Canonical Fetcher Scheduler
# ------------------------------------------------------------
async def schedule_canonical_fetcher(interval_sec: int = 300):
    print(f"[canon_sched] üöÄ Canonical fetch scheduler every {interval_sec}s")
    while True:
        try:
            print("[canon_sched] üîÅ Running canonical_fetcher_run_once()")
            await canonical_fetcher_run_once()
        except Exception as e:
            print(f"[canon_sched] üî• Error: {e}")

        print(f"[canon_sched] ‚è≥ Sleeping {interval_sec}s")
        await asyncio.sleep(interval_sec)


# ------------------------------------------------------------
# Raw Article Fetching
# ------------------------------------------------------------
async def schedule_article_fetching(interval_sec: int = 45):
    r = redis.Redis(host="127.0.0.1", port=6381, decode_responses=True)
    print(f"[article_raw] üöÄ Raw fetcher (interval={interval_sec}s)")

    while True:
        try:
            uids = await r.zrevrange("rss:index", 0, 50)

            if not uids:
                print("[article_raw] üí§ No items in rss:index")
                await asyncio.sleep(interval_sec)
                continue

            for uid in uids:
                item_key = f"rss:item:{uid}"
                raw_key = f"rss:article_raw:{uid}"

                if await r.exists(raw_key):
                    continue

                item = await r.hgetall(item_key)
                if not item:
                    continue

                url = item.get("url")
                if not url:
                    continue

                print(f"[article_raw] üåê Fetching {uid[:8]} ‚Üí {url}")

                await fetch_and_store_article(
                    uid=uid,
                    url=url,
                    title=item.get("title", ""),
                    category=item.get("category", ""),
                    r_intel=r,
                )

            await asyncio.sleep(interval_sec)

        except Exception as e:
            print(f"[article_raw] üî• Error: {e}")
            await asyncio.sleep(interval_sec)


# ------------------------------------------------------------
# Enrichment
# ------------------------------------------------------------
async def schedule_article_enrichment(interval_sec: int):
    while True:
        try:
            print("[enrich] üìù Running enrichment cycle‚Ä¶")
            await enrich_articles()
        except Exception as e:
            print(f"[Enrichment Error] {e}")

        await asyncio.sleep(interval_sec)


# ------------------------------------------------------------
# Tier-0 Ingestor
# ------------------------------------------------------------
async def start_workflow(svc, feeds_conf):
    redis_host = os.getenv("SYSTEM_REDIS_HOST", "127.0.0.1")
    redis_port = int(os.getenv("SYSTEM_REDIS_PORT", "6379"))

    print(f"[debug] Connecting to Redis {redis_host}:{redis_port}")

    try:
        r = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)
        await r.ping()
        print(f"[debug] ‚úÖ Connected to Redis {redis_host}:{redis_port}")
    except Exception as e:
        raise ConnectionError(f"[Redis Error] Could not connect: {e}")

    interval = feeds_conf["workflow"].get("schedule_sec", 600)

    print(f"üöÄ Starting RSS ingest workflow for {svc}")
    print(f"‚è±Ô∏è Every {interval}s")

    # Operator feedback for force mode
    if FORCE_INGEST:
        print("‚ö° [ingestor] FORCE_INGEST active ‚Äî ignoring rss:seen filter")

    while True:
        try:
            print("üì° Starting feed ingestion‚Ä¶")
            await ingest_feeds(feeds_conf)

            await r.hset("rss_agg:status", mapping={
                "last_run_ts": time.time(),
                "last_status": "success",
            })

        except Exception as e:
            await r.hset("rss_agg:status", mapping={
                "last_run_ts": time.time(),
                "last_status": f"failed: {e}",
            })
            print(f"[RSS Agg Error] {e}")

        await asyncio.sleep(interval)


# ------------------------------------------------------------
# ORCHESTRATOR ENTRYPOINT
# ------------------------------------------------------------
async def run_orchestrator(svc: str, setup_info: dict, truth: dict):
    print(f"[orchestrator:init] üß† Orchestrator starting ({PIPELINE_MODE})")

    feeds_cfg = setup_info["feeds_cfg"]
    intel_info = setup_info["intel_redis"]

    publish_dir = truth["components"][svc]["workflow"]["publish_dir"]
    interval = feeds_cfg["workflow"].get("schedule_sec", 600)

    # Validate intel-redis
    r_intel = redis.Redis(
        host=intel_info["host"],
        port=intel_info["port"],
        decode_responses=True,
    )
    await r_intel.ping()

    print(f"[orchestrator] [ok] Connected to intel-redis at "
          f"{intel_info['host']}:{intel_info['port']}")

    # Show force mode
    if FORCE_INGEST:
        print("‚ö° [orchestrator] FORCE_INGEST=true ‚Äî ingestor will reprocess all URLs")

    # ----------------- MODES -----------------

    if PIPELINE_MODE == "ingest_only":
        print("[orchestrator] üî¨ Ingest-only")
        await asyncio.gather(start_workflow(svc, feeds_cfg))
        return

    if PIPELINE_MODE == "canonical_only":
        print("[orchestrator] üî¨ Canonical-only")
        await asyncio.gather(schedule_canonical_fetcher(300))
        return

    if PIPELINE_MODE == "fetch_only":
        print("[orchestrator] üî¨ Raw fetch-only")
        await asyncio.gather(schedule_article_fetching(30))
        return

    if PIPELINE_MODE == "enrich_only":
        print("[orchestrator] üî¨ Enrichment-only")
        await asyncio.gather(enrich_articles_lifo(30))
        return

    if PIPELINE_MODE == "publish_only":
        print("[orchestrator] üî¨ Publish-only")
        await asyncio.gather(schedule_feed_generation(publish_dir, interval))
        return

    # ---------------- FULL PIPELINE -----------------
    print("[orchestrator] üöÄ FULL PIPELINE MODE")

    await asyncio.gather(
        start_workflow(svc, feeds_cfg),
        schedule_canonical_fetcher(300),
        schedule_article_fetching(30),
        enrich_articles_lifo(30),
        schedule_feed_generation(publish_dir, interval),
    )